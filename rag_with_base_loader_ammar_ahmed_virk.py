# -*- coding: utf-8 -*-
"""RAG with base loader_Ammar Ahmed Virk.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PQZSMyNsJAQQ0FyIUqc2wl7aHacwPwSB
"""

!pip install -q langchain openai faiss-cpu tiktoken langchain_community gradio
!pip install -q beautifulsoup4 html2text

from langchain.document_loaders import WebBaseLoader

# üåç List of URLs to include
urls = [
    "https://nyuad.nyu.edu/en/research/faculty.html"
]

# Load the content
loader = WebBaseLoader(urls)
docs = loader.load()

import os
os.environ["OPENAI_API_KEY"] = "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"  # üîê Replace with your OpenAI API Key

import os
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain_text_splitters import CharacterTextSplitter

embedding = OpenAIEmbeddings()

# Check if vector DB exists
if os.path.exists("faiss_index"):
    db = FAISS.load_local("faiss_index", embeddings=embedding,allow_dangerous_deserialization="True")
else:
    # Load, chunk, and embed from scratch
    loader = WebBaseLoader(urls)
    docs = loader.load()
    split_docs = CharacterTextSplitter(chunk_size=800, chunk_overlap=100).split_documents(docs)
    db = FAISS.from_documents(split_docs, embedding)
    db.save_local("faiss_index")

from langchain.prompts import PromptTemplate

template = """
Use the following context to answer the question. Be as specific and accurate as possible.

Context:
{context}

Question:
{question}

Answer:
"""

prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=template,
)

from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=db.as_retriever(),
    chain_type_kwargs={"prompt": prompt}
)

import gradio as gr

# üí¨ Function to handle user query
def chat(query):
    return qa_chain.run(query)

# üñºÔ∏è Gradio Interface
gr.Interface(
    fn=chat,
    inputs=gr.Textbox(lines=2, placeholder="Ask about any professor related to any majors...", label="Your Question"),
    outputs=gr.Textbox(label="Answer"),
    title="üîé Player' placement finder",
    description="Ask about any professor related to any majors."
).launch(share=True)

